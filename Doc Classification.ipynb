{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAB14.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5tVZVT8Vd90"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# загружаем корпус из файла\n",
        "x = []\n",
        "y = []\n",
        "classes_count = {}\n",
        "with open('corp0.txt', 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        label = re.match(r'#\\w+', line).group()\n",
        "        y.append(label[1:])\n",
        "        try:\n",
        "            classes_count[label[1:]] += 1\n",
        "        except(KeyError):\n",
        "            classes_count[label[1:]] = 1\n",
        "        x.append(re.sub(label+' ', '', line))\n",
        "x = np.array(x)\n",
        "y = np.array(y)\n",
        "\n",
        "print(y[0])\n",
        "print(x[0])\n",
        "\n",
        "print('Размерность x: {}'.format(x.shape))\n",
        "print('Размерность y: {}'.format(y.shape))\n",
        "print('Распределение документов по классам:\\n {}'.format(classes_count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqwtGQwAf--K",
        "outputId": "ed9e9f1b-9ce3-4b56-e5aa-ed8544c3a608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "семья\n",
            "Ваши дети совсем не хотят вылезать из реки. С каждым днем \"допустимое\" время просмотра телевизора становится все больше и больше. Вы не уверены, что способны вынести хотя бы еще один крик \"Его половина больше моей!\"\n",
            "\n",
            "Размерность x: (3400,)\n",
            "Размерность y: (3400,)\n",
            "Распределение документов по классам:\n",
            " {'семья': 101, 'реклама': 94, 'недвижимость': 98, 'здоровье': 157, 'политика': 600, 'культура': 358, 'спорт': 373, 'техника': 289, 'экономика': 272, 'происшествия': 436, 'автомобили': 249, 'страна': 146, 'наука': 227}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "обработка корпуса"
      ],
      "metadata": {
        "id": "25MMvhyTnFY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2[fast]"
      ],
      "metadata": {
        "id": "UeSfIlySR61U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64282cf7-89f0-4520-b6c2-e1d8efd476f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy2[fast]\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 16.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51 kB 10.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 55 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2[fast]) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2 MB 10.5 MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting DAWG>=0.8\n",
            "  Downloading DAWG-0.8.0.tar.gz (371 kB)\n",
            "\u001b[K     |████████████████████████████████| 371 kB 40.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: DAWG\n",
            "  Building wheel for DAWG (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for DAWG: filename=DAWG-0.8.0-cp37-cp37m-linux_x86_64.whl size=865923 sha256=b871dd7c44b17372899ee3196e57e9c04652cda29a9a06c31f268cbb438efc1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/51/a4/2de41ff197786537075027c27b479a38da92f50abc86634445\n",
            "Successfully built DAWG\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2, DAWG\n",
            "Successfully installed DAWG-0.8.0 dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "print(stopwords.words('russian'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZdAB8ms5hMb",
        "outputId": "8cd86306-534a-4e2a-965c-d2f0ed6a3b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pymorphy2\n",
        "\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def uniqueWords(x):\n",
        "    uniquewords = set()\n",
        "    for sentence in x:\n",
        "        tokenized_sentence = sentence.split()\n",
        "        for word in tokenized_sentence:\n",
        "            uniquewords.add(word)\n",
        "    return len(uniquewords)\n",
        "\n",
        "uniqueWordsBefore = uniqueWords(x)\n",
        "print('Количество уникальных символов до обработки: {}'.format(uniqueWordsBefore))\n",
        "\n",
        "sentence = x[0]\n",
        "\n",
        "sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "print('Убираем пунктуацию: {}'.format(sentence))\n",
        "\n",
        "sentence = word_tokenize(sentence)\n",
        "print('Разбиваем на токены: {}'.format(sentence))\n",
        "\n",
        "sentence = [word for word in sentence if word not in stopwords.words('russian')]\n",
        "print('Убираем стоп-слова: {}'.format(sentence))\n",
        "\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "sentence = [morph.parse(word)[0].normal_form for word in sentence]\n",
        "print('Лемматизация: {}'.format(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d2Vw8bBnFH5",
        "outputId": "66a0a872-7909-4e4a-e990-d93e0182ca62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество уникальных символов до обработки: 140077\n",
            "Убираем пунктуацию: Ваши дети совсем не хотят вылезать из реки С каждым днем допустимое время просмотра телевизора становится все больше и больше Вы не уверены что способны вынести хотя бы еще один крик Его половина больше моей\n",
            "\n",
            "Разбиваем на токены: ['Ваши', 'дети', 'совсем', 'не', 'хотят', 'вылезать', 'из', 'реки', 'С', 'каждым', 'днем', 'допустимое', 'время', 'просмотра', 'телевизора', 'становится', 'все', 'больше', 'и', 'больше', 'Вы', 'не', 'уверены', 'что', 'способны', 'вынести', 'хотя', 'бы', 'еще', 'один', 'крик', 'Его', 'половина', 'больше', 'моей']\n",
            "Убираем стоп-слова: ['Ваши', 'дети', 'хотят', 'вылезать', 'реки', 'С', 'каждым', 'днем', 'допустимое', 'время', 'просмотра', 'телевизора', 'становится', 'Вы', 'уверены', 'способны', 'вынести', 'хотя', 'крик', 'Его', 'половина', 'моей']\n",
            "Лемматизация: ['ваш', 'ребёнок', 'хотеть', 'вылезать', 'река', 'с', 'каждый', 'день', 'допустимый', 'время', 'просмотр', 'телевизор', 'становиться', 'вы', 'уверить', 'способный', 'вынести', 'хотя', 'крик', 'он', 'половина', 'мой']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentence(sentence, stopwords=stopwords.words('russian'), morph=pymorphy2.MorphAnalyzer()):\n",
        "    clean_sentence = []\n",
        "\n",
        "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "    sentence = re.sub('[a-zA-Z]\\w+', 'FRGN', sentence)\n",
        "    sentence = re.sub('\\d+', 'NUMB', sentence)\n",
        "    # tokenized_sentence = word_tokenize(sentence)[0:25]\n",
        "    tokenized_sentence = sentence.split()\n",
        "    for word in tokenized_sentence:\n",
        "        if word not in stopwords:\n",
        "            clean_sentence.append(morph.parse(word)[0].normal_form)\n",
        "\n",
        "    clean_sentence = ' '.join([word for word in clean_sentence])\n",
        "\n",
        "    return clean_sentence\n",
        "\n",
        "\n",
        "for i in range(len(x)):\n",
        "    x[i] = process_sentence(x[i])\n",
        "\n",
        "uniqueWordsAfter = uniqueWords(x)\n",
        "print('Количество уникальных символов после обработки: {}'.format(uniqueWordsAfter))"
      ],
      "metadata": {
        "id": "_aS6AqY7UVX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693da62f-5cbc-4450-d778-11b0f5781d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество уникальных символов после обработки: 10251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(token_pattern='\\w+')\n",
        "x_data = vectorizer.fit_transform(x)\n",
        "x_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnuz4sKcZzjC",
        "outputId": "30d44aae-2bf2-4bfc-e0fa-e21846de6f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3400x10251 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 60508 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y, test_size=0.25, random_state=42)\n",
        "\n",
        "print('Размерность обучающего множества: {}, {}'.format(x_train.shape, y_train.shape))\n",
        "print('Размерность тестового множества: {}, {}'.format(x_test.shape, y_test.shape))"
      ],
      "metadata": {
        "id": "yr-LzqMmDqwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676f23a3-9e2e-47e8-9db1-2741fd445d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность обучающего множества: (2550, 10251), (2550,)\n",
            "Размерность тестового множества: (850, 10251), (850,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_cls = 3\n",
        "\n",
        "if sklearn_cls == 0:\n",
        "    from sklearn.linear_model import SGDClassifier # stochastic gradient descent\n",
        "    doc_clf = SGDClassifier(loss = 'hinge', max_iter = 1000, tol = 1e-3)\n",
        "elif sklearn_cls == 1:\n",
        "    from sklearn.ensemble import AdaBoostClassifier\n",
        "    doc_clf = AdaBoostClassifier()\n",
        "elif sklearn_cls == 2:\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    doc_clf = RandomForestClassifier()\n",
        "elif sklearn_cls == 3:\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    doc_clf = LogisticRegression(solver = 'lbfgs', # newton-cg\n",
        "                                max_iter = 500, multi_class = 'auto')\n",
        "    \n",
        "%time doc_clf.fit(x_train, y_train)\n",
        "\n",
        "score_test = doc_clf.score(x_test, y_test)\n",
        "print('Оценка точности классификации: {}%'.format(round(score_test * 100, 4)))\n",
        "\n",
        "score_train = doc_clf.score(x_train, y_train)\n",
        "print('Точность классификации на тренировочном множестве: {}%'.format(round(score_train * 100, 4)))"
      ],
      "metadata": {
        "id": "Nl0TDXj90hGW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39732ae0-7cdd-4588-aa8b-0dd0e8559115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.75 s, sys: 4.5 s, total: 9.25 s\n",
            "Wall time: 5.6 s\n",
            "Оценка точности классификации: 81.6471%\n",
            "Точность классификации на проверочном множестве: 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predict = doc_clf.predict(x_test)\n",
        "print(classification_report(y_test, predict, digits=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX9bTGSMcSuq",
        "outputId": "ea064ba0-a02f-4eeb-b2d7-6f83f2a43097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "  автомобили     0.8281    0.7910    0.8092        67\n",
            "    здоровье     0.7955    0.6604    0.7216        53\n",
            "    культура     0.7714    0.8710    0.8182        93\n",
            "       наука     0.8600    0.8958    0.8776        48\n",
            "недвижимость     0.7500    0.5357    0.6250        28\n",
            "    политика     0.8235    0.9197    0.8690       137\n",
            "происшествия     0.8716    0.8879    0.8796       107\n",
            "     реклама     0.9048    0.6333    0.7451        30\n",
            "       семья     0.7037    0.7600    0.7308        25\n",
            "       спорт     0.9091    0.9639    0.9357        83\n",
            "      страна     0.5556    0.3947    0.4615        38\n",
            "     техника     0.8143    0.8028    0.8085        71\n",
            "   экономика     0.7778    0.8000    0.7887        70\n",
            "\n",
            "    accuracy                         0.8165       850\n",
            "   macro avg     0.7973    0.7628    0.7747       850\n",
            "weighted avg     0.8133    0.8165    0.8115       850\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nCwGw_dgfJ76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}